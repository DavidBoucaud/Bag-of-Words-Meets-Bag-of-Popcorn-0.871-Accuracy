{
 "cells": [
  {
   "source": [
    "# Bag of Words Meets Bag of Popcorn Submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's import some basic libraries to get things rolling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "source": [
    "Alright, on to cleaning the text lceaning. Some of these methods are a bit redundant, but after a few trials I found that this best cleans all the noise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "\n",
    "def clean_me(text):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += list(string.punctuation)\n",
    "    token = word_tokenize\n",
    "    tokens = list(token(text))\n",
    "    stopwords_removed = [token.lower() for token in tokens if not token.lower() in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in stopwords_removed])"
   ]
  },
  {
   "source": [
    "Now to import the libraries and clean the review column right away. Naturally, I took out my information but you can replace the string with your file paths real easy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I downloaded the data and saved it. It can now be loaded with this method.\n",
    "df = pd.read_table(r'XX')\n",
    "test_df = pd.read_table(r'XX')\n",
    "#Let's clean it right away\n",
    "df.review = df.review.apply(clean_me)\n",
    "test_df.review = test_df.review.apply(clean_me)"
   ]
  },
  {
   "source": [
    "Alright now onto the actual work. I always build a parameter grid with many features then print out the best parameters to speed up the process in the future. For the sake of transparency, I left the whole grid here.\n",
    "\n",
    "Admittedly, I tried many classification methods (MNB, KClusters, Random Forest) but simple Logistics Regression returned the highest accuracy score.\n",
    "\n",
    "For further transparency, let's run a train-test split so that we can see the working model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.sentiment, test_size = 0.1, random_state=0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('norma', MaxAbsScaler()),\n",
    "    ('reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'tfidf__analyzer' : ['word', 'char'],\n",
    "    'reg__class_weight': ['balanced', None],\n",
    "    'reg__fit_intercept': [True, False]\n",
    "    }]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=7)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "pred = grid.predict(X_test)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(confusion_matrix(pred, y_test))\n",
    "print(accuracy_score(pred, y_test))\n",
    "\n",
    "#This takes a minute to run. When I did it, it came back with an 0.898 accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('norma', MaxAbsScaler()),\n",
    "    ('reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'tfidf__analyzer' : ['word', 'char'],\n",
    "    'reg__class_weight': ['balanced', None],\n",
    "    'reg__fit_intercept': [True, False]\n",
    "    }]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10)\n",
    "grid.fit(df.review, df.sentiment)\n",
    "\n",
    "pred = grid.predict(test_df.review)\n",
    "\n",
    "test_df['sentiment'] = pred"
   ]
  },
  {
   "source": [
    "To close off, let's make the submission file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(test_df[['id', 'sentiment']])\n",
    "sub_df.to_csv('submission', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             id  sentiment\n",
       "0      12311_10          1\n",
       "1        8348_2          0\n",
       "2        5828_4          0\n",
       "3        7186_2          1\n",
       "4       12128_7          1\n",
       "...         ...        ...\n",
       "24995   2155_10          1\n",
       "24996     59_10          1\n",
       "24997    2531_1          0\n",
       "24998    7772_8          1\n",
       "24999  11465_10          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12311_10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8348_2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5828_4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7186_2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12128_7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>2155_10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>59_10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>2531_1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>7772_8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>11465_10</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "pd.read_csv(r'C:\\Users\\David\\Documents\\code\\Mod_5\\Bag of Popcorn\\submission')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}